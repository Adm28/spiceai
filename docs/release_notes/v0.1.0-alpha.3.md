# Spice AI v0.1.0-alpha.3

We're happy to announce the release of Spice AI v0.1.0-alpha.3!

## New in this release

- **Added** a new [sample](https://github.com/spiceai/samples/tree/trunk/log-pruner) and [quickstart](https://github.com/spiceai/quickstarts/tree/trunk/log-pruner) for Spice AI based on figuring out the best time to prune logs on a server
- **Added** a new [sample](https://github.com/spiceai/samples/tree/trunk/kubernetes) that shows how to get Spice AI running in your Kubernetes cluster
- **Refactored** the way that data is specified in the pod manifest file (see notes below)
- **Fixes** an issue where deleting a manifest file was not handled gracefully by the Spice AI runtime
- **Refactored** logging to not show debug logs in the output by default
- **Refactored** the communication between the Spice AI runtime and the AI Engine to use gRPC instead of HTTP
- **Fixed** the Spice runtime running in Docker to respect the `http_port` config value

## Notes

One of the major changes in this update is a refactoring of how data sources are specified in the pod manifest. Previously the way to specify a data source in the pod manifest looked like this:

```yaml
- datasource:
  from: coinbase
  name: btcusd
  connector:
    type: csv
    params:
      path: data/btcusd.csv
```

It has been changed to:

```yaml
- datasource:
  from: coinbase
  name: btcusd
  data:
    connector:
      name: file
      params:
        path: ../../test/assets/data/csv/COINBASE_BTCUSD, 30.csv
    processor:
      name: csv
```

This change decouples the `connector` (the code that actually fetches the data) from the `processor` (the code that transforms the fetched data into our observation format). This allows a user to get data from say a file or a blob, and process it in csv format. We expect an ecosystem will be built of data connectors and data processors.
